{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1wGl0fsIZkAa"
   },
   "source": [
    "# 3 Exploring Big Data (Optional Technical Task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JsrjebLMZrI_"
   },
   "source": [
    "This is the Third(optional) taksk of the Data@ANZ Virtual Experiance program.  \n",
    "Tasks : \n",
    "1. Create Spark application using both the RDD and Dataframe API through different classes or functions.\n",
    "\n",
    "2. Using each API, perform the following transformation steps using the synthetic transaction file as input referenced as an input argument to your program. Output the results to a local file.\n",
    "\n",
    "Project only the records where status=authorized AND card_present_flag=0\n",
    "Split the long_lat and merchant_long_lat fields into long, lat and merch_long, merch_lat fields\n",
    "Output the data as a CSV file\n",
    "3. Time each operation and report on the differences in execution time between each API. Print the output using stdout in your program. \n",
    "\n",
    "You are free to develop your application using any Spark first class programming language, preferably Scala or Java however.\n",
    "\n",
    "While designing your program and code, make sure your program is robust, maintainable and scalable. \n",
    "\n",
    "4. For a challenge!\n",
    "\n",
    "Create a program to read each line in the file and perform the same steps using a Spark Streaming (or Structured Streaming) application, using the console as output.\n",
    "\n",
    "Submit your report (in pdf format)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PG2haenCGwMW",
    "outputId": "dd51ce0e-d6c2-4889-e6be-205c6e637979"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "/content/drive/My Drive/Colab Notebooks/spark_data\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "os.chdir('/content/drive/My Drive/Colab Notebooks/spark_data/')\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_ovVl0SAZqLF",
    "outputId": "6f5c2e72-81ca-4123-f126-08b34c753a03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
      "Hit:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
      "Ign:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
      "Hit:4 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
      "Hit:5 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
      "Ign:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
      "Hit:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
      "Hit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
      "Get:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
      "Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
      "Hit:11 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
      "Get:12 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
      "Hit:13 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
      "Fetched 252 kB in 4s (70.5 kB/s)\n",
      "Reading package lists... Done\n"
     ]
    }
   ],
   "source": [
    "!apt-get update\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "!wget -q http://archive.apache.org/dist/spark/spark-2.3.1/spark-2.3.1-bin-hadoop2.7.tgz\n",
    "!tar xf spark-2.3.1-bin-hadoop2.7.tgz\n",
    "!pip install -q findspark\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.3.1-bin-hadoop2.7\"\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate() \n",
    "spark\n",
    "\n",
    "from pyspark.sql import SQLContext \n",
    "sql = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "MoXFwhBWDS64"
   },
   "outputs": [],
   "source": [
    "# from google.colab import files\n",
    "# uploaded = files.upload()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ta07AlXccyqP",
    "outputId": "7be90c27-0d5a-4ad2-9781-df9a0435bda0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BIGDATA_Content_Task.xlsx  spark-2.3.1-bin-hadoop2.7.tgz\n",
      "spark-2.3.1-bin-hadoop2.7  spark-2.3.1-bin-hadoop2.7.tgz.1\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6T0wU8DKLCtO"
   },
   "source": [
    "## Using DataFrame API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pc4KZADiaXTR",
    "outputId": "eef4e728-e1b7-4476-8c84-1c46b05ce248"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+----------------+--------------+--------+-------------+---------------+--------------------+-------------+----------+-------+-------------------+------+---+---------------+--------------+--------------------+------+--------------------+---------+--------------+-----------------+--------+\n",
      "|    status|card_present_flag|bpay_biller_code|       account|currency|     long_lat|txn_description|         merchant_id|merchant_code|first_name|balance|               date|gender|age|merchant_suburb|merchant_state|          extraction|amount|      transaction_id|  country|   customer_id|merchant_long_lat|movement|\n",
      "+----------+-----------------+----------------+--------------+--------+-------------+---------------+--------------------+-------------+----------+-------+-------------------+------+---+---------------+--------------+--------------------+------+--------------------+---------+--------------+-----------------+--------+\n",
      "|authorized|              1.0|             NaN|ACC-1598451071|     AUD|153.41 -27.95|            POS|81c48296-73be-44a...|          NaN|     Diana|  35.39|2018-08-01 00:00:00|     F| 26|        Ashmore|           QLD|2018-08-01T01:01:...| 16.25|a623070bfead4541a...|Australia|CUS-2487424745|    153.38 -27.99|   debit|\n",
      "|authorized|              0.0|             NaN|ACC-1598451071|     AUD|153.41 -27.95|      SALES-POS|830a451c-316e-4a6...|          NaN|     Diana|   21.2|2018-08-01 00:00:00|     F| 26|         Sydney|           NSW|2018-08-01T01:13:...| 14.19|13270a2a902145da9...|Australia|CUS-2487424745|    151.21 -33.87|   debit|\n",
      "|authorized|              1.0|             NaN|ACC-1222300524|     AUD|151.23 -33.94|            POS|835c231d-8cdf-4e9...|          NaN|   Michael|   5.71|2018-08-01 00:00:00|     M| 38|         Sydney|           NSW|2018-08-01T01:26:...|  6.42|feb79e7ecd7048a5a...|Australia|CUS-2142601169|    151.21 -33.87|   debit|\n",
      "|authorized|              1.0|             NaN|ACC-1037050564|     AUD|153.10 -27.66|      SALES-POS|48514682-c78a-4a8...|          NaN|    Rhonda|2117.22|2018-08-01 00:00:00|     F| 40|        Buderim|           QLD|2018-08-01T01:38:...|  40.9|2698170da3704fd98...|Australia|CUS-1614226872|    153.05 -26.68|   debit|\n",
      "|authorized|              1.0|             NaN|ACC-1598451071|     AUD|153.41 -27.95|      SALES-POS|b4e02c10-0852-427...|          NaN|     Diana|  17.95|2018-08-01 00:00:00|     F| 26|  Mermaid Beach|           QLD|2018-08-01T01:51:...|  3.25|329adf79878c4cf0a...|Australia|CUS-2487424745|    153.44 -28.06|   debit|\n",
      "+----------+-----------------+----------------+--------------+--------+-------------+---------------+--------------------+-------------+----------+-------+-------------------+------+---+---------------+--------------+--------------------+------+--------------------+---------+--------------+-----------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, DoubleType, StringType, TimestampType, BooleanType, IntegerType\n",
    "import pandas as pd\n",
    "\n",
    "stk = [\n",
    "('status',StringType()), \n",
    "('card_present_flag',StringType()),\n",
    "('bpay_biller_code',StringType()),\n",
    "('account',StringType()),\n",
    "('currency',StringType()), \n",
    "('long_lat',StringType()), \n",
    "('txn_description',StringType()), \n",
    "('merchant_id',StringType()),\n",
    "('merchant_code',StringType()), \n",
    "('first_name',StringType()), \n",
    "('balance',DoubleType()), \n",
    "('date',TimestampType()), \n",
    "('gender',StringType()), \n",
    "('age',IntegerType()),\n",
    "('merchant_suburb',StringType()), \n",
    "('merchant_state',StringType()), \n",
    "('extraction',StringType()), \n",
    "('amount',DoubleType()),\n",
    "('transaction_id',StringType()), \n",
    "('country',StringType()), \n",
    "('customer_id',StringType()), \n",
    "('merchant_long_lat',StringType()),\n",
    "('movement',StringType())]\n",
    "\n",
    "schema = StructType([StructField(x[0], x[1],True) for x in stk])\n",
    "data = pd.read_excel('BIGDATA_Content_Task.xlsx')\n",
    "df = spark.createDataFrame(data, schema = schema) \n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "4qJqBTEhwyzt"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, substring, split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MCOzqX7QxOHt"
   },
   "source": [
    "### records with status = authorized and card_present_flag = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wp_ZfDiGqQ7f",
    "outputId": "24d252e4-04a1-4c64-badc-7d225e057c9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+----------------+--------------+--------+-------------+---------------+--------------------+-------------+----------+--------+-------------------+------+---+---------------+--------------+--------------------+------+--------------------+---------+--------------+-----------------+--------+\n",
      "|    status|card_present_flag|bpay_biller_code|       account|currency|     long_lat|txn_description|         merchant_id|merchant_code|first_name| balance|               date|gender|age|merchant_suburb|merchant_state|          extraction|amount|      transaction_id|  country|   customer_id|merchant_long_lat|movement|\n",
      "+----------+-----------------+----------------+--------------+--------+-------------+---------------+--------------------+-------------+----------+--------+-------------------+------+---+---------------+--------------+--------------------+------+--------------------+---------+--------------+-----------------+--------+\n",
      "|authorized|              0.0|             NaN|ACC-1598451071|     AUD|153.41 -27.95|      SALES-POS|830a451c-316e-4a6...|          NaN|     Diana|    21.2|2018-08-01 00:00:00|     F| 26|         Sydney|           NSW|2018-08-01T01:13:...| 14.19|13270a2a902145da9...|Australia|CUS-2487424745|    151.21 -33.87|   debit|\n",
      "|authorized|              0.0|             NaN|ACC-2890243754|     AUD|153.32 -27.93|            POS|7e8bf667-e724-435...|          NaN|    Joseph|  275.93|2018-08-01 00:00:00|     M| 37|        Lismore|           NSW|2018-08-01T08:19:...| 24.77|1f12467d33ce46098...|Australia|CUS-2695611575|    153.28 -28.81|   debit|\n",
      "|authorized|              0.0|             NaN|ACC-2615038700|     AUD|145.35 -38.03|            POS|354f40cb-55bc-4a8...|          NaN|     Emily|30583.15|2018-08-01 00:00:00|     F| 43|     Mordialloc|           VIC|2018-08-01T08:47:...| 12.08|49417bad354f41379...|Australia|CUS-3255104878|    145.09 -38.01|   debit|\n",
      "|authorized|              0.0|             NaN|ACC-1710017148|     AUD|150.82 -34.01|      SALES-POS|4af25042-a1a4-468...|          NaN|  Michelle| 1625.34|2018-08-01 00:00:00|     F| 19|     Alexandria|           NSW|2018-08-01T09:11:...| 11.57|82acf03790844776b...|Australia| CUS-883482547|    151.19 -33.92|   debit|\n",
      "|authorized|              0.0|             NaN|ACC-3485804958|     AUD|138.52 -35.01|            POS|a08935a2-99a8-49f...|          NaN|   Jessica|12529.59|2018-08-01 00:00:00|     F| 34|         Findon|            SA|2018-08-01T09:19:...| 33.89|89050ee5c5aa4e79b...|Australia|CUS-1196156254|     138.53 -34.9|   debit|\n",
      "+----------+-----------------+----------------+--------------+--------+-------------+---------------+--------------------+-------------+----------+--------+-------------------+------+---+---------------+--------------+--------------------+------+--------------------+---------+--------------+-----------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1523"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auth_card = df.filter((col('status')=='authorized') &  (col('card_present_flag')=='0.0'))\n",
    "auth_card.show(5)\n",
    "auth_card.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ieub7B8VxbaM"
   },
   "source": [
    "### splitting the merchant_long_lat and long_lat in to seperate columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "grLL9zs3yGms",
    "outputId": "cb0ca858-e6e7-4e36-99c4-521832d9e61d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function split in module pyspark.sql.functions:\n",
      "\n",
      "split(str, pattern)\n",
      "    Splits str around pattern (pattern is a regular expression).\n",
      "    \n",
      "    .. note:: pattern is a string represent the regular expression.\n",
      "    \n",
      "    >>> df = spark.createDataFrame([('ab12cd',)], ['s',])\n",
      "    >>> df.select(split(df.s, '[0-9]+').alias('s')).collect()\n",
      "    [Row(s=['ab', 'cd'])]\n",
      "    \n",
      "    .. versionadded:: 1.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GcNX2J4nyfxt",
    "outputId": "70718252-6108-4ab1-a15d-76d6ba4b0e8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+----------------+--------------+--------+-------------+---------------+--------------------+-------------+----------+--------+-------------------+------+---+---------------+--------------+--------------------+------+--------------------+---------+--------------+-----------------+--------+------+------+----------+---------+\n",
      "|    status|card_present_flag|bpay_biller_code|       account|currency|     long_lat|txn_description|         merchant_id|merchant_code|first_name| balance|               date|gender|age|merchant_suburb|merchant_state|          extraction|amount|      transaction_id|  country|   customer_id|merchant_long_lat|movement|  long|   lat|merch_long|merch_lat|\n",
      "+----------+-----------------+----------------+--------------+--------+-------------+---------------+--------------------+-------------+----------+--------+-------------------+------+---+---------------+--------------+--------------------+------+--------------------+---------+--------------+-----------------+--------+------+------+----------+---------+\n",
      "|authorized|              0.0|             NaN|ACC-1598451071|     AUD|153.41 -27.95|      SALES-POS|830a451c-316e-4a6...|          NaN|     Diana|    21.2|2018-08-01 00:00:00|     F| 26|         Sydney|           NSW|2018-08-01T01:13:...| 14.19|13270a2a902145da9...|Australia|CUS-2487424745|    151.21 -33.87|   debit|153.41|-27.95|    151.21|   -33.87|\n",
      "|authorized|              0.0|             NaN|ACC-2890243754|     AUD|153.32 -27.93|            POS|7e8bf667-e724-435...|          NaN|    Joseph|  275.93|2018-08-01 00:00:00|     M| 37|        Lismore|           NSW|2018-08-01T08:19:...| 24.77|1f12467d33ce46098...|Australia|CUS-2695611575|    153.28 -28.81|   debit|153.32|-27.93|    153.28|   -28.81|\n",
      "|authorized|              0.0|             NaN|ACC-2615038700|     AUD|145.35 -38.03|            POS|354f40cb-55bc-4a8...|          NaN|     Emily|30583.15|2018-08-01 00:00:00|     F| 43|     Mordialloc|           VIC|2018-08-01T08:47:...| 12.08|49417bad354f41379...|Australia|CUS-3255104878|    145.09 -38.01|   debit|145.35|-38.03|    145.09|   -38.01|\n",
      "|authorized|              0.0|             NaN|ACC-1710017148|     AUD|150.82 -34.01|      SALES-POS|4af25042-a1a4-468...|          NaN|  Michelle| 1625.34|2018-08-01 00:00:00|     F| 19|     Alexandria|           NSW|2018-08-01T09:11:...| 11.57|82acf03790844776b...|Australia| CUS-883482547|    151.19 -33.92|   debit|150.82|-34.01|    151.19|   -33.92|\n",
      "|authorized|              0.0|             NaN|ACC-3485804958|     AUD|138.52 -35.01|            POS|a08935a2-99a8-49f...|          NaN|   Jessica|12529.59|2018-08-01 00:00:00|     F| 34|         Findon|            SA|2018-08-01T09:19:...| 33.89|89050ee5c5aa4e79b...|Australia|CUS-1196156254|     138.53 -34.9|   debit|138.52|-35.01|    138.53|    -34.9|\n",
      "+----------+-----------------+----------------+--------------+--------+-------------+---------------+--------------------+-------------+----------+--------+-------------------+------+---+---------------+--------------+--------------------+------+--------------------+---------+--------------+-----------------+--------+------+------+----------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "auth_card = auth_card.withColumn('long',split(auth_card['long_lat'],' ').getItem(0))\n",
    "auth_card = auth_card.withColumn('lat',split(auth_card['long_lat'],' ').getItem(1))\n",
    "auth_card = auth_card.withColumn('merch_long',split(auth_card['merchant_long_lat'],' ').getItem(0))\n",
    "auth_card = auth_card.withColumn('merch_lat',split(auth_card['merchant_long_lat'],' ').getItem(1))\n",
    "\n",
    "auth_card.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "CRaYQXQtzT3a"
   },
   "outputs": [],
   "source": [
    "# auth_card.write.format(\"csv\").save(\"auth_card_dataframe.csv\")\n",
    "# files.download(\"auth_card_dataframe.csv\")\n",
    "\n",
    "auth_card_pd = auth_card.toPandas()\n",
    "auth_card_pd.to_csv('auth_card_dataframe.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tUVX4qzb8JR_",
    "outputId": "5ee8f9f0-6147-4631-965f-19a9679698c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auth_card_dataframe.csv    spark-2.3.1-bin-hadoop2.7.tgz\n",
      "BIGDATA_Content_Task.xlsx  spark-2.3.1-bin-hadoop2.7.tgz.1\n",
      "spark-2.3.1-bin-hadoop2.7\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ev8OjYC4LYPb"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "3_ExploreBigData.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
